{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-institute",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ethereumetl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Mainnet RPC\n",
    "provider_uri = 'YOUR_MAINNET_RPC_ENDPOINT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-kingston",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last block of each calendar year\n",
    "last_block_2015 = 778482\n",
    "last_block_2016 = 2912406\n",
    "last_block_2017 = 4832685\n",
    "last_block_2018 = 6988614\n",
    "last_block_2019 = 9193265\n",
    "last_block_2020 = 11565018\n",
    "last_block_2021 = 13916165\n",
    "last_block_2022 = 15884996 # Through Nov 2, 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2015 contract deployers\n",
    "start_block = 40000 # No transactions before block 40,000\n",
    "end_block = start_block + 9999\n",
    "\n",
    "contract_deployers = []\n",
    "\n",
    "while (end_block <= last_block_2015+10000): # Buffer based on chunk size\n",
    "    end_block_used = min(end_block,last_block_2015)\n",
    "    print(\"Fetching blocks \"+str(start_block)+ \" to \"+str(end_block_used)+\".\")\n",
    "    ! ethereumetl export_blocks_and_transactions --start-block $start_block --end-block $end_block_used \\\n",
    "    --provider-uri $provider_uri \\\n",
    "    --transactions-output transactions.csv\n",
    "\n",
    "    transactions = pd.read_csv('transactions.csv')\n",
    "    \n",
    "    # Get contract deployers\n",
    "    transactions = transactions[transactions['to_address'].isnull()]\n",
    "    contract_deployers = contract_deployers + transactions['from_address'].unique().tolist()\n",
    "    \n",
    "    # Delete tempfile and increment block range\n",
    "    os.remove(\"transactions.csv\")\n",
    "    start_block = start_block + 10000\n",
    "    end_block = end_block + 10000\n",
    "\n",
    "# Remove duplicates across chunks\n",
    "unique_contract_deployers = [*set(contract_deployers)]\n",
    "\n",
    "# Export to CSV\n",
    "unique_contract_deployers_output = pd.DataFrame(unique_contract_deployers, columns=[\"contract_deployers\"])\n",
    "unique_contract_deployers_output.to_csv('mainnet_unique_contract_deployers_2015.csv', index=False)\n",
    "\n",
    "print(\"Unique Contract Deployers (2015): \"+str(len(unique_contract_deployers)))\n",
    "\n",
    "# Alternate Dune Query\n",
    "# select distinct \"from\" as contract_deployers from ethereum.transactions\n",
    "# where \"block_number\" > 0 --Genesis block\n",
    "# and \"block_number\" <= 778482 --Last block in 2015\n",
    "# and \"to\" is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2016 contract deployers\n",
    "start_block = last_block_2015+1\n",
    "end_block = start_block + 9999\n",
    "\n",
    "contract_deployers = []\n",
    "\n",
    "while (end_block <= last_block_2016+10000): # Buffer based on chunk size\n",
    "    end_block_used = min(end_block,last_block_2016)\n",
    "    print(\"Fetching blocks \"+str(start_block)+ \" to \"+str(end_block_used)+\".\")\n",
    "    ! ethereumetl export_blocks_and_transactions --start-block $start_block --end-block $end_block_used \\\n",
    "    --provider-uri $provider_uri \\\n",
    "    --transactions-output transactions.csv\n",
    "\n",
    "    transactions = pd.read_csv('transactions.csv')\n",
    "    \n",
    "    #Get contract deployers\n",
    "    transactions = transactions[transactions['to_address'].isnull()]\n",
    "    contract_deployers = contract_deployers + transactions['from_address'].unique().tolist()\n",
    "    \n",
    "    #Delete tempfile and increment block range\n",
    "    os.remove(\"transactions.csv\")\n",
    "    start_block = start_block + 10000\n",
    "    end_block = end_block + 10000\n",
    "\n",
    "#Remove duplicates across chunks\n",
    "unique_contract_deployers = [*set(contract_deployers)]\n",
    "\n",
    "# Export to CSV\n",
    "unique_contract_deployers_output = pd.DataFrame(unique_contract_deployers, columns=[\"contract_deployers\"])\n",
    "unique_contract_deployers_output.to_csv('mainnet_unique_contract_deployers_2016.csv', index=False)\n",
    "\n",
    "print(\"Unique Contract Deployers (2016): \"+str(len(unique_contract_deployers)))\n",
    "\n",
    "# Alternate Dune Query\n",
    "# select distinct \"from\" as contract_deployers from ethereum.transactions\n",
    "# where \"block_number\" > 778482 --First block in 2015\n",
    "# and \"block_number\" <= 2912406 --Last block in 2015\n",
    "# and \"to\" is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-barrel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2017 contract deployers\n",
    "start_block = last_block_2016+1\n",
    "end_block = start_block + 9999\n",
    "\n",
    "contract_deployers = []\n",
    "\n",
    "while (end_block <= last_block_2017+10000): # Buffer based on chunk size\n",
    "    end_block_used = min(end_block,last_block_2017)\n",
    "    print(\"Fetching blocks \"+str(start_block)+ \" to \"+str(end_block_used)+\".\")\n",
    "    ! ethereumetl export_blocks_and_transactions --start-block $start_block --end-block $end_block_used \\\n",
    "    --provider-uri $provider_uri \\\n",
    "    --transactions-output transactions.csv\n",
    "\n",
    "    transactions = pd.read_csv('transactions.csv')\n",
    "    \n",
    "    #Get contract deployers\n",
    "    transactions = transactions[transactions['to_address'].isnull()]\n",
    "    contract_deployers = contract_deployers + transactions['from_address'].unique().tolist()\n",
    "    \n",
    "    #Delete tempfile and increment block range\n",
    "    os.remove(\"transactions.csv\")\n",
    "    start_block = start_block + 10000\n",
    "    end_block = end_block + 10000\n",
    "\n",
    "#Remove duplicates across chunks\n",
    "unique_contract_deployers = [*set(contract_deployers)]\n",
    "\n",
    "# Export to CSV\n",
    "unique_contract_deployers_output = pd.DataFrame(unique_contract_deployers, columns=[\"contract_deployers\"])\n",
    "unique_contract_deployers_output.to_csv('mainnet_unique_contract_deployers_2017.csv', index=False)\n",
    "\n",
    "print(\"Unique Contract Deployers (2017): \"+str(len(unique_contract_deployers)))\n",
    "\n",
    "# Alternate Dune Query\n",
    "# select distinct \"from\" as contract_deployers from ethereum.transactions\n",
    "# where \"block_number\" > 2912406 --First block in 2017\n",
    "# and \"block_number\" <= 4832685 --Last block in 2017\n",
    "# and \"to\" is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2018 contract deployers\n",
    "start_block = last_block_2017+1\n",
    "end_block = start_block + 9999\n",
    "\n",
    "contract_deployers = []\n",
    "\n",
    "while (end_block <= last_block_2018+10000): # Buffer based on chunk size\n",
    "    end_block_used = min(end_block,last_block_2018)\n",
    "    print(\"Fetching blocks \"+str(start_block)+ \" to \"+str(end_block_used)+\".\")\n",
    "    ! ethereumetl export_blocks_and_transactions --start-block $start_block --end-block $end_block_used \\\n",
    "    --provider-uri $provider_uri \\\n",
    "    --transactions-output transactions.csv\n",
    "\n",
    "    transactions = pd.read_csv('transactions.csv')\n",
    "    \n",
    "    #Get contract deployers\n",
    "    transactions = transactions[transactions['to_address'].isnull()]\n",
    "    contract_deployers = contract_deployers + transactions['from_address'].unique().tolist()\n",
    "    \n",
    "    #Delete tempfile and increment block range\n",
    "    os.remove(\"transactions.csv\")\n",
    "    start_block = start_block + 10000\n",
    "    end_block = end_block + 10000\n",
    "\n",
    "#Remove duplicates across chunks\n",
    "unique_contract_deployers = [*set(contract_deployers)]\n",
    "\n",
    "# Export to CSV\n",
    "unique_contract_deployers_output = pd.DataFrame(unique_contract_deployers, columns=[\"contract_deployers\"])\n",
    "unique_contract_deployers_output.to_csv('mainnet_unique_contract_deployers_2018.csv', index=False)\n",
    "\n",
    "print(\"Unique Contract Deployers (2018): \"+str(len(unique_contract_deployers)))\n",
    "\n",
    "# Alternate Dune Query\n",
    "# select distinct \"from\" as contract_deployers from ethereum.transactions\n",
    "# where \"block_number\" > 4832685 --First block in 2018\n",
    "# and \"block_number\" <= 6988614 --Last block in 2018\n",
    "# and \"to\" is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019 contract deployers\n",
    "start_block = last_block_2018+1\n",
    "end_block = start_block + 9999\n",
    "\n",
    "contract_deployers = []\n",
    "\n",
    "while (end_block <= last_block_2019+10000): # Buffer based on chunk size\n",
    "    end_block_used = min(end_block,last_block_2019)\n",
    "    print(\"Fetching blocks \"+str(start_block)+ \" to \"+str(end_block_used)+\".\")\n",
    "    ! ethereumetl export_blocks_and_transactions --start-block $start_block --end-block $end_block_used \\\n",
    "    --provider-uri $provider_uri \\\n",
    "    --transactions-output transactions.csv\n",
    "\n",
    "    transactions = pd.read_csv('transactions.csv')\n",
    "    \n",
    "    #Get contract deployers\n",
    "    transactions = transactions[transactions['to_address'].isnull()]\n",
    "    contract_deployers = contract_deployers + transactions['from_address'].unique().tolist()\n",
    "    \n",
    "    #Delete tempfile and increment block range\n",
    "    os.remove(\"transactions.csv\")\n",
    "    start_block = start_block + 10000\n",
    "    end_block = end_block + 10000\n",
    "\n",
    "#Remove duplicates across chunks\n",
    "unique_contract_deployers = [*set(contract_deployers)]\n",
    "\n",
    "# Export to CSV\n",
    "unique_contract_deployers_output = pd.DataFrame(unique_contract_deployers, columns=[\"contract_deployers\"])\n",
    "unique_contract_deployers_output.to_csv('mainnet_unique_contract_deployers_2019.csv', index=False)\n",
    "\n",
    "print(\"Unique Contract Deployers (2019): \"+str(len(unique_contract_deployers)))\n",
    "\n",
    "# Alternate Dune Query\n",
    "# select distinct \"from\" as contract_deployers from ethereum.transactions\n",
    "# where \"block_number\" > 6988614 --First block in 2019\n",
    "# and \"block_number\" <= 9193265 --Last block in 2019\n",
    "# and \"to\" is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2020 contract deployers\n",
    "start_block = last_block_2019+1\n",
    "end_block = start_block + 9999\n",
    "\n",
    "contract_deployers = []\n",
    "\n",
    "while (end_block <= last_block_2020+10000): # Buffer based on chunk size\n",
    "    end_block_used = min(end_block,last_block_2020)\n",
    "    print(\"Fetching blocks \"+str(start_block)+ \" to \"+str(end_block_used)+\".\")\n",
    "    ! ethereumetl export_blocks_and_transactions --start-block $start_block --end-block $end_block_used \\\n",
    "    --provider-uri $provider_uri \\\n",
    "    --transactions-output transactions.csv\n",
    "\n",
    "    transactions = pd.read_csv('transactions.csv')\n",
    "    \n",
    "    #Get contract deployers\n",
    "    transactions = transactions[transactions['to_address'].isnull()]\n",
    "    contract_deployers = contract_deployers + transactions['from_address'].unique().tolist()\n",
    "    \n",
    "    #Delete tempfile and increment block range\n",
    "    os.remove(\"transactions.csv\")\n",
    "    start_block = start_block + 10000\n",
    "    end_block = end_block + 10000\n",
    "\n",
    "#Remove duplicates across chunks\n",
    "unique_contract_deployers = [*set(contract_deployers)]\n",
    "\n",
    "# Export to CSV\n",
    "unique_contract_deployers_output = pd.DataFrame(unique_contract_deployers, columns=[\"contract_deployers\"])\n",
    "unique_contract_deployers_output.to_csv('mainnet_unique_contract_deployers_2020.csv', index=False)\n",
    "\n",
    "print(\"Unique Contract Deployers (2020): \"+str(len(unique_contract_deployers)))\n",
    "\n",
    "# Alternate Dune Query\n",
    "# select distinct \"from\" as contract_deployers from ethereum.transactions\n",
    "# where \"block_number\" > 9193265 --First block in 2020\n",
    "# and \"block_number\" <= 11565018 --Last block in 2020\n",
    "# and \"to\" is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-lying",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2021 contract deployers\n",
    "start_block = last_block_2020+1\n",
    "end_block = start_block + 9999\n",
    "\n",
    "contract_deployers = []\n",
    "\n",
    "while (end_block <= last_block_2021+10000): # Buffer based on chunk size\n",
    "    end_block_used = min(end_block,last_block_2021)\n",
    "    print(\"Fetching blocks \"+str(start_block)+ \" to \"+str(end_block_used)+\".\")\n",
    "    ! ethereumetl export_blocks_and_transactions --start-block $start_block --end-block $end_block_used \\\n",
    "    --provider-uri $provider_uri \\\n",
    "    --transactions-output transactions.csv\n",
    "\n",
    "    transactions = pd.read_csv('transactions.csv')\n",
    "    \n",
    "    #Get contract deployers\n",
    "    transactions = transactions[transactions['to_address'].isnull()]\n",
    "    contract_deployers = contract_deployers + transactions['from_address'].unique().tolist()\n",
    "    \n",
    "    #Delete tempfile and increment block range\n",
    "    os.remove(\"transactions.csv\")\n",
    "    start_block = start_block + 10000\n",
    "    end_block = end_block + 10000\n",
    "\n",
    "#Remove duplicates across chunks\n",
    "unique_contract_deployers = [*set(contract_deployers)]\n",
    "\n",
    "# Export to CSV\n",
    "unique_contract_deployers_output = pd.DataFrame(unique_contract_deployers, columns=[\"contract_deployers\"])\n",
    "unique_contract_deployers_output.to_csv('mainnet_unique_contract_deployers_2021.csv', index=False)\n",
    "\n",
    "print(\"Unique Contract Deployers (2021): \"+str(len(unique_contract_deployers)))\n",
    "\n",
    "# Alternate Dune Query\n",
    "# select distinct \"from\" as contract_deployers from ethereum.transactions\n",
    "# where \"block_number\" > 11565018 --First block in 2021\n",
    "# and \"block_number\" <= 13916165 --Last block in 2021\n",
    "# and \"to\" is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-insured",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022 contract deployers\n",
    "start_block = last_block_2021+1\n",
    "end_block = start_block + 9999\n",
    "\n",
    "contract_deployers = []\n",
    "\n",
    "while (end_block <= last_block_2022+10000): # Buffer based on chunk size\n",
    "    end_block_used = min(end_block,last_block_2022)\n",
    "    print(\"Fetching blocks \"+str(start_block)+ \" to \"+str(end_block_used)+\".\")\n",
    "    ! ethereumetl export_blocks_and_transactions --start-block $start_block --end-block $end_block_used \\\n",
    "    --provider-uri $provider_uri \\\n",
    "    --transactions-output transactions.csv\n",
    "\n",
    "    transactions = pd.read_csv('transactions.csv')\n",
    "    \n",
    "    #Get contract deployers\n",
    "    transactions = transactions[transactions['to_address'].isnull()]\n",
    "    contract_deployers = contract_deployers + transactions['from_address'].unique().tolist()\n",
    "    \n",
    "    #Delete tempfile and increment block range\n",
    "    os.remove(\"transactions.csv\")\n",
    "    start_block = start_block + 10000\n",
    "    end_block = end_block + 10000\n",
    "\n",
    "#Remove duplicates across chunks\n",
    "unique_contract_deployers = [*set(contract_deployers)]\n",
    "\n",
    "# Export to CSV\n",
    "unique_contract_deployers_output = pd.DataFrame(unique_contract_deployers, columns=[\"contract_deployers\"])\n",
    "unique_contract_deployers_output.to_csv('mainnet_unique_contract_deployers_2022.csv', index=False)\n",
    "\n",
    "print(\"Unique Contract Deployers (2022): \"+str(len(unique_contract_deployers)))\n",
    "\n",
    "# Alternate Dune Query\n",
    "# select distinct \"from\" as contract_deployers from ethereum.transactions\n",
    "# where \"block_number\" > 13916165 --First block in 2022\n",
    "# and \"block_number\" <= 15884996 --Through Nov 2, 2022\n",
    "# and \"to\" is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-ecology",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_deployers_2015 = pd.read_csv('mainnet_unique_contract_deployers_2015.csv')\n",
    "contract_deployers_2016 = pd.read_csv('mainnet_unique_contract_deployers_2016.csv')\n",
    "contract_deployers_2017 = pd.read_csv('mainnet_unique_contract_deployers_2017.csv')\n",
    "contract_deployers_2018 = pd.read_csv('mainnet_unique_contract_deployers_2018.csv')\n",
    "contract_deployers_2019 = pd.read_csv('mainnet_unique_contract_deployers_2019.csv')\n",
    "contract_deployers_2020 = pd.read_csv('mainnet_unique_contract_deployers_2020.csv')\n",
    "contract_deployers_2021 = pd.read_csv('mainnet_unique_contract_deployers_2021.csv')\n",
    "contract_deployers_2022 = pd.read_csv('mainnet_unique_contract_deployers_2022.csv')\n",
    "\n",
    "#Generate CSV for full time range\n",
    "contract_deployers_all = contract_deployers_2015.append(contract_deployers_2016).append(contract_deployers_2017).append(contract_deployers_2018).append(contract_deployers_2019).append(contract_deployers_2020).append(contract_deployers_2021).append(contract_deployers_2022)\n",
    "unique_contract_deployers_all = contract_deployers_all['contract_deployers'].unique()\n",
    "unique_contract_deployers_all_df = pd.DataFrame(unique_contract_deployers_all, columns=['contract_deployers'])\n",
    "unique_contract_deployers_all_df.to_csv('mainnet_unique_contract_deployers_all.csv', index=False)\n",
    "\n",
    "#Calculate Scores: 1 point for each calendar year in which an address deployed a contract\n",
    "mainnet_scores = contract_deployers_all.assign(Count=1).groupby('contract_deployers')['Count'].count().reset_index()\n",
    "\n",
    "#Show score distribution\n",
    "mainnet_scores_summary = mainnet_scores.groupby('Count').count()\n",
    "display(mainnet_scores_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chart of unique contract deployers over time\n",
    "year = [\"2015\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\",\"2021\",\"2022\"]\n",
    "deployers = [len(contract_deployers_2015.index),len(contract_deployers_2016.index),len(contract_deployers_2017.index),len(contract_deployers_2018.index),len(contract_deployers_2019.index),len(contract_deployers_2020.index),len(contract_deployers_2021.index),len(contract_deployers_2022.index)]\n",
    "\n",
    "df = pd.DataFrame({'contract deployers': deployers}, index=year)\n",
    "\n",
    "axes = df.plot.bar(rot=0, subplots=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
